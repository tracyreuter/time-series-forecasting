{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting with MSTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tracy.reuter/anaconda3/lib/python3.11/site-packages/statsforecast/core.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# load libraries and data\n",
    "import os\n",
    "os.environ[\"NIXTLA_ID_AS_COL\"] = \"1\" # suppress future warning\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import MSTL, AutoARIMA, SeasonalNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and wrangle data (original data source: https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption)\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/tracyreuter/time-series-forecasting-mstl/main/electric_load_hourly.csv')\n",
    "df.columns = ['ds', 'y']\n",
    "df.insert(0, 'unique_id', 'PJM_Load_hourly')\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MSTL model parameters\n",
    "mstl = MSTL(\n",
    "    # seasonalities of the time series\n",
    "    season_length=[24, 24 * 7],\n",
    "    # model used to forecast trend\n",
    "    trend_forecaster=AutoARIMA()\n",
    ")\n",
    "sf = StatsForecast(\n",
    "    # models used to forecast time series\n",
    "    models=[mstl,\n",
    "    # benchmark against a SeasonalNaive model\n",
    "    SeasonalNaive(season_length=24)],\n",
    "    # data are hourly\n",
    "    freq='h'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train data, using the last 24 hours for testing\n",
    "df_test = df.tail(24)\n",
    "df_train = df.drop(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the train data\n",
    "sf = sf.fit(df=df_train)\n",
    "# generate forecasts for the test data\n",
    "# horizon (h) is the length of the test data\n",
    "forecasts_test = sf.predict(h=len(df_test), level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compare model performance visually\n",
    "def plot_forecasts(y_hist, y_true, y_pred, models):\n",
    "    _, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n",
    "    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(24 * 7)\n",
    "    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n",
    "    colors = ['orange', 'green', 'red']\n",
    "    # omit shading to better see point estimates\n",
    "    # for model, color in zip(models, colors):\n",
    "    #     ax.fill_between(df_plot.index,\n",
    "    #                     df_plot[f'{model}-lo-90'],\n",
    "    #                     df_plot[f'{model}-hi-90'],\n",
    "    #                     alpha=.35,\n",
    "    #                     color=color,\n",
    "    #                     label=f'{model}-level-90')\n",
    "    ax.set_title('Actual and Forecasted Load', fontsize=15)\n",
    "    ax.set_ylabel('Electricity Load', fontsize=15)\n",
    "    ax.set_xlabel('Timestamp', fontsize=15)\n",
    "    ax.legend(prop={'size': 10})\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare model performance visually\n",
    "plot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import measures for model performance\n",
    "from datasetsforecast.losses import (\n",
    "    mae, mape, mase, rmse, smape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compare model performance numerically\n",
    "def evaluate_performace(y_hist, y_true, y_pred, models):\n",
    "    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n",
    "    evaluation = {}\n",
    "    for model in models:\n",
    "        evaluation[model] = {}\n",
    "        for metric in [mase, mae, mape, rmse, smape]:\n",
    "            metric_name = metric.__name__\n",
    "            if metric_name == 'mase':\n",
    "                evaluation[model][metric_name] = metric(y_true['y'].values,\n",
    "                                                 y_true[model].values,\n",
    "                                                 y_hist['y'].values, seasonality=24)\n",
    "            else:\n",
    "                evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n",
    "    return pd.DataFrame(evaluation).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare model performance numerically\n",
    "evaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare MSTL versus Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify prophet model\n",
    "from prophet import Prophet\n",
    "# prophet interval_width=0.90 == statsforecast level=[90]\n",
    "prophet = Prophet(interval_width=0.90)\n",
    "# fit model\n",
    "prophet.fit(df_train)\n",
    "# generate forecasts\n",
    "future = prophet.make_future_dataframe(periods=len(df_test), freq='h', include_history=False)\n",
    "forecast_prophet = prophet.predict(future)\n",
    "# wrangle data\n",
    "forecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "forecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\n",
    "forecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\n",
    "forecast_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join forecast results\n",
    "forecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare model performance visually\n",
    "plot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare model performance numerically\n",
    "evaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n",
    "# MSTL outperforms SeasonalNaive and Prophet models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
